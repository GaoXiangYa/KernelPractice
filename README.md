# KernelPractice
Practice how to write high performance kernels

## CUDA

- [ ] FlashAttention
- [ ] LayerNorm
- [x] RMSNorm
- [ ] Split
- [ ] Cat
- [x] Gemm
- [ ] Gemv
- [ ] SoftMax
- [ ] Gelu
- [ ] Silu
- [ ] Swiglu
- [ ] Add
- [ ] Mul
- [ ] Permute
- [ ] LlamaRotatePosition2D
- [x] Reduce

## CPU

- [ ] FlashAttention
- [ ] LayerNorm
- [] RMSNorm
- [ ] Split
- [ ] Cat
- [x] Gemm
- [ ] Gemv
- [ ] SoftMax
- [ ] Gelu
- [ ] Silu
- [ ] Swiglu
- [ ] Add
- [ ] Mul
- [ ] Permute
- [ ] LlamaRotatePosition2D
- [ ] Reduce
