# KernelPractice
Practice how to write high performance kernels

## CUDA

- [ ] Attention
- [ ] FlashAttention
- [ ] LayerNorm
- [ ] Embedding
- [x] RMSNorm
- [ ] Split
- [ ] Cat
- [x] Gemm
- [ ] SoftMax
- [ ] Gelu
- [ ] Silu
- [ ] Swiglu
- [ ] Add
- [ ] Mul
- [ ] Permute
- [ ] LlamaRotatePosition2D
- [x] Reduce
