# KernelPractice
Practice how to write high performance kernels

## CUDA

- [ ] FlashAttention
- [ ] LayerNorm
- [x] RMSNorm
- [ ] Split
- [ ] Cat
- [x] Gemm
- [x] Gemv
- [ ] SoftMax
- [ ] Gelu
- [ ] Silu
- [ ] Swiglu
- [ ] Add
- [ ] Mul
- [ ] Permute
- [ ] LlamaRotatePosition2D
- [x] Reduce

## CPU

- [ ] FlashAttention
- [ ] LayerNorm
- [ ] RMSNorm
- [ ] Split
- [ ] Cat
- [x] Gemm
- [ ] Gemv
- [ ] SoftMax
- [ ] Gelu
- [ ] Silu
- [ ] Swiglu
- [ ] Add
- [ ] Mul
- [ ] Permute
- [ ] LlamaRotatePosition2D
- [ ] Reduce

# OPENCL

- [ ] FlashAttention
- [ ] LayerNorm
- [ ] RMSNorm
- [ ] Split
- [ ] Cat
- [ ] Gemm
- [ ] Gemv
- [ ] SoftMax
- [ ] Gelu
- [ ] Silu
- [ ] Swiglu
- [ ] Add
- [ ] Mul
- [ ] Permute
- [ ] LlamaRotatePosition2D
- [x] Reduce
